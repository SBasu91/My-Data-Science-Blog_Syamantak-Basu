[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/PARI Dataset Code/index.html",
    "href": "posts/PARI Dataset Code/index.html",
    "title": "PARI Dataset Code",
    "section": "",
    "text": "This is a post with executable code.\nThe dataset for this code is derived from the website of the public non-profit citizen-run archive known as the People’s Archive of Rural India (PARI). I collected this dataset by writing a code scraper in Python, with the help of I-School professor Dr Zoe LeBlanc, for the class “Culture at Scale.” The scraper is designed to collect the headlines of all the stories gathered under one category on the website known as “Farming and its Crisis: The Troubled World of Agriculture.” The scraper collected 348 stories hosted in the category between July 18, 2014 and April 29, 2023 (since my analyses were conducted right after that date). The website, and in particular this category, may be found at https://ruralindiaonline.org/en/stories/categories/farming-and-its-crisis/\nThe website is open to all contributors, with stories vetted by a team of volunteer editors, and collects stories directly from the people of rural India. It is therefore one of India’s most important citizen archives which amplifies voices from the margins. The material is licensed under a Creative Commons CC 4.0 license, and credits ownership first to the people whose stories are being told, and thereafter to those recording and submitting them. The stories are available in multiple languages, so the scraper was modified to only collect data in English.\nI used this as a counterpoint to the Indian newspaper headlines dataset (which compiles headlines from India’s major English newspaper), which is why I chose the words farming, digital and poverty for the correlation analysis in the Indian News Headlines Dataset.\nPhoto credit: M. Palani Kumar\n\n# I scraped and curated this dataset, with the help of Dr Zoe LeBlanc\ndata &lt;- read.csv(\"/Users/syamantakbasu91/Desktop/IS 407 assignments/My Data Science Blog_Syamantak Basu/PARI Compiled and Filtered.csv\")\n\n\n# Setting a CRAN repository directly for the tm package\ncran_mirror &lt;- \"https://cloud.r-project.org/\"\n\n# Installing the 'tm' package with the specified CRAN mirror\ninstall.packages(\"tm\", repos = cran_mirror)\n\n\nThe downloaded binary packages are in\n    /var/folders/8b/kr1x3k6n1gs6fzmv7qjyzx5h0000gn/T//RtmpCkVekm/downloaded_packages\n\n# Loading the necessary libraries\nlibrary(tm)\n\nLoading required package: NLP\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n# Caption_text is the column which has the titles of the stories\ntext_data &lt;- data$caption_text\n\n# Tokenizing the text\ncorpus &lt;- Corpus(VectorSource(text_data))\ncorpus &lt;- tm_map(corpus, content_transformer(tolower))\n\nWarning in tm_map.SimpleCorpus(corpus, content_transformer(tolower)):\ntransformation drops documents\n\ncorpus &lt;- tm_map(corpus, removePunctuation)\n\nWarning in tm_map.SimpleCorpus(corpus, removePunctuation): transformation drops\ndocuments\n\ncorpus &lt;- tm_map(corpus, removeNumbers)\n\nWarning in tm_map.SimpleCorpus(corpus, removeNumbers): transformation drops\ndocuments\n\ncorpus &lt;- tm_map(corpus, removeWords, stopwords(\"english\"))\n\nWarning in tm_map.SimpleCorpus(corpus, removeWords, stopwords(\"english\")):\ntransformation drops documents\n\ncorpus &lt;- tm_map(corpus, stripWhitespace)\n\nWarning in tm_map.SimpleCorpus(corpus, stripWhitespace): transformation drops\ndocuments\n\n# Creating a DocumentTermMatrix\ndtm &lt;- DocumentTermMatrix(corpus)\n\n# Converting the DocumentTermMatrix to a matrix\nmatrix &lt;- as.matrix(dtm)\n\n# Calculating the word frequencies\nword_freq &lt;- colSums(matrix)\n\n# Sorting the word frequencies in descending order\nsorted_freq &lt;- sort(word_freq, decreasing = TRUE)\n\n# Finding the top 5 words besides the single quote\ntop_words &lt;- head(sorted_freq[!names(sorted_freq) %in% \"‘\"], 5) #In the first iteration, \"'\" was the most frequent word, so I modified the code to remove it and then show the next five words\n\n# Printing the top 5 words\nprint(top_words)\n\nfarmers    farm    will  farmer   march \n     41      16      12      11      11 \n\n\n\n# Visualizing the top 5 occurring words\n# Loading the ggplot library\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:NLP':\n\n    annotate\n\n# Creating a data frame for the bar plot\ntop_words_df &lt;- data.frame(word = names(top_words), frequency = top_words)\n\n# Defining a color palette for the bars\ncolors &lt;- rainbow(length(top_words_df$word))\n\n# Plotting a bar graph with different colors for each bar\nggplot(top_words_df, aes(x = word, y = frequency, fill = word)) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Word\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Top 5 Most Occurring Words\") +\n  scale_fill_manual(values = colors) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n# Finding the correlation coefficients of the top 5 words\n# Calculating the correlations between the top 5 words\nword_correlations &lt;- cor(matrix[, names(top_words)])\n\n# Printing the correlation matrix\nprint(word_correlations)\n\n            farmers        farm        will      farmer       march\nfarmers  1.00000000 -0.03766696 -0.01866649 -0.06602435 -0.06602435\nfarm    -0.03766696  1.00000000 -0.03831188 -0.03966179  0.11718257\nwill    -0.01866649 -0.03831188  1.00000000 -0.03153000  0.05159454\nfarmer  -0.06602435 -0.03966179 -0.03153000  1.00000000 -0.03264095\nmarch   -0.06602435  0.11718257  0.05159454 -0.03264095  1.00000000\n\n\n\n# Creating a scatterplot with color gradients to map the correlations between the top 5 words\n\ninstall.packages(\"tidyr\", repos = cran_mirror)\n\n\nThe downloaded binary packages are in\n    /var/folders/8b/kr1x3k6n1gs6fzmv7qjyzx5h0000gn/T//RtmpCkVekm/downloaded_packages\n\nlibrary(tidyr)\n\n\n\n# Creating a data frame from the correlation matrix\ncorrelation_df &lt;- as.data.frame(word_correlations)\ncorrelation_df$word1 &lt;- rownames(correlation_df)\nrownames(correlation_df) &lt;- NULL\ncorrelation_df &lt;- gather(correlation_df, key = \"word2\", value = \"correlation\", -word1)\n\n# Creating scatterplots for each correlation\nscatterplots &lt;- ggplot(correlation_df, aes(x = word1, y = word2, color = correlation)) +\n  geom_point(size = 3) +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(title = \"Word Correlations\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Displaying the scatterplots\nprint(scatterplots)"
  },
  {
    "objectID": "posts/Indian News Headlines Dataset Code/index.html",
    "href": "posts/Indian News Headlines Dataset Code/index.html",
    "title": "Indian News Headlines Dataset",
    "section": "",
    "text": "This is a post with executable code.\nFor this post I will be using the Indian News Headlines Dataset, an open-source dataset which is a “persistent historical archive of noteable events in the Indian subcontinent from start-2001 to q1-2022, recorded in real-time by the journalists of India” and contains approx 3.6 million headline events. The dataset was prepared by Rohit Kulkarni and is available in the public domain with a CCO 1.0 DEED license (universal).\nThe dataset is periodically updated and may be found here:https://www.kaggle.com/datasets/therohk/india-headlines-news-dataset\nPhoto credit: Markus Spike on Pixabay (https://pixabay.com/photos/newspaper-coffee-vintage-retro-595478/)\n\ndata &lt;- read.csv(\"/Users/syamantakbasu91/Desktop/IS 407 assignments/My Data Science Blog_Syamantak Basu/india-news-headlines.csv\")\n\n\n# I want to explore the correlations between the relationship between the words poverty and digital (with Digital India being a major buzzword since 2014), poverty and farming, and farming and digital. Thereafter I will visualize the correlation through bar graphs.\n\n# Preprocessing the dataset to count word occurrences\nword1 &lt;- 'poverty'\nword2 &lt;- 'digital'\nword3 &lt;- 'farming'\n\ndata$word1_count &lt;- sapply(data$headline_text, function(text) sum(grepl(word1, text, ignore.case = TRUE)))\ndata$word2_count &lt;- sapply(data$headline_text, function(text) sum(grepl(word2, text, ignore.case = TRUE)))\ndata$word3_count &lt;- sapply(data$headline_text, function(text) sum(grepl(word3, text, ignore.case = TRUE)))\n\n# Calculating the correlations between the three words\ncorrelation_word1_word2 &lt;- cor(data$word1_count, data$word2_count)\ncorrelation_word1_word3 &lt;- cor(data$word1_count, data$word3_count)\ncorrelation_word2_word3 &lt;- cor(data$word2_count, data$word3_count)\n\n# Visualizing the correlations\n# Loading the ggplot2 library\nlibrary(ggplot2)\n\ncorrelation_data &lt;- data.frame(\n  Pair = c(\"word1-word2\", \"word1-word3\", \"word2-word3\"),\n  Correlation = c(correlation_word1_word2, correlation_word1_word3, correlation_word2_word3),\n  Word_Pair = c(paste(word1, \"-\", word2), paste(word1, \"-\", word3), paste(word2, \"-\", word3))\n)\n\n# Creating a named vector for custom colors\ncolor_mapping &lt;- setNames(c(\"red\", \"blue\", \"green\"), c(\n  paste(word1, \"-\", word2),\n  paste(word1, \"-\", word3),\n  paste(word2, \"-\", word3)\n))\n\n# Creating a grouped bar chart with actual word pair names in the legend\nggplot(correlation_data, aes(x = Pair, y = Correlation, fill = Word_Pair)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Correlations between Word Pairs\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_manual(values = color_mapping) +\n  guides(fill = guide_legend(title = \"Word Pair\"))\n\n\n\n\n\n#Finding the top 3 words occurring in the headlines\n# Loading the necessary libraries\n\nlibrary(tidytext)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Tokenizing the text into words\ndata %&gt;%\n  mutate(headline_text = tolower(headline_text)) %&gt;%  # Converting to lowercase for case insensitivity\n  unnest_tokens(word, headline_text, token = \"words\") %&gt;%\n\n  # Filtering out common stopwords if needed\n  anti_join(stop_words) %&gt;%\n\n  # Counting word occurrences\n  count(word, sort = TRUE) %&gt;%\n\n  # Selecting the top three words\n  head(3)\n\nJoining with `by = join_by(word)`\n\n\n   word     n\n1    rs 83121\n2 india 76216\n3  govt 73385\n\n\n\n#Visualizing the top 3 words as a scatterplot\n# Tokenizing the text into words and counting word occurrences\nword_frequencies &lt;- data %&gt;%\n  mutate(headline_text = tolower(headline_text)) %&gt;%  # Converting to lowercase for case insensitivity\n  unnest_tokens(word, headline_text, token = \"words\") %&gt;%\n  # Filtering out common stopwords if needed\n  anti_join(stop_words) %&gt;%\n  count(word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\n# Selecting the top three words\ntop_words &lt;- head(word_frequencies, 3)\n\n# Defining a color palette for the top three words\nword_colors &lt;- c(\"red\", \"blue\", \"green\")\n\n# Creating a scatterplot of the top three words with colors\nggplot(top_words, aes(x = reorder(word, -n), y = n, fill = word)) +\n  geom_point(shape = 21, size = 4) +  # Customizing shape and size of points\n  scale_fill_manual(values = word_colors) +  # Assigning colors to words\n  labs(x = \"Word\", y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n# Visualizing the top 3 words together with the three custom words of my choosing as a scatterplot\n\n# Defining the additional words\nadditional_words &lt;- c(\"poverty\", \"farming\", \"digital\")\n\n# Filtering and counting the frequencies of the additional words\nadditional_word_frequencies &lt;- data %&gt;%\n  mutate(headline_text = tolower(headline_text)) %&gt;%\n  unnest_tokens(word, headline_text, token = \"words\") %&gt;%\n  filter(word %in% additional_words) %&gt;%\n  count(word, sort = TRUE)\n\n# Combining the top words and additional words\nall_words &lt;- rbind(top_words, additional_word_frequencies)\n\n# Defining a color palette for the words\nword_colors &lt;- c(\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"cyan\")\n\n# Creating a scatterplot of all words with colors\n\nggplot(all_words, aes(x = reorder(word, -n), y = n, fill = word)) +\n  geom_point(shape = 21, size = 4) +\n  scale_fill_manual(values = word_colors) +\n  labs(x = \"Word\", y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n# Visualizing the occurences of all six words as a histogram\n\nggplot(all_words, aes(x = word, y = n, fill = word)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = word_colors) +  # Assigning colors to the words\n  labs(x = \"Word\", y = \"Frequency\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/About Me/index.html",
    "href": "posts/About Me/index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Syamantakshobhan Basu, and this is my Data Science Blog\nI’m a 2nd-year PhD student in the Department of English at the University of Illinois at Urbana-Champaign. A recent immigrant, my home country is India, and I carry an interest in a wide range of subjects into my degree. Having studied English literature at the undergraduate and graduate levels, I’ve branched out into film and media studies, digital humanities and for the last two years have been training myself in methods and approaches in Information Science. I hope to combine critical theory and IS/DH methods to critically analyze the relationship between the technosphere, nationalism and neoliberalism/globalization in India post 1947. Before starting my PhD I used to work as a copyeditor and proofreader at an academic publishing house, and I hope to use those skills in my PhD as well.\nThrough my coursework I’ve been able to achieve a good balance between literary and sociological theory and methods in Digital Humanities. As Illinois Distinguished Fellow, I hope to utilize the resources available to me to do more meaningful research bridging the humanities and technology studies.\nBesides academics, I’m interested in films (both watching and making them), video games and music. I don’t read as much outside of work as I used to, but I still love science fiction and fantasy, and I love football to death (I mean the European kind, I refuse to call it soccer!) I also play the guitar and used to sing in a band. I miss those days sometimes, but maybe I’ll get a chance sometime soon! I’m also a Star Wars fanatic, and enjoy watching everything SW with a group of like-minded fans every week.\nLooking forward to recording many evenings of cats, coding and coffee here.\n-Syamantak\nNov 7, 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Data Science Blog_Syamantak Basu",
    "section": "",
    "text": "About Me\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nSyamantak Basu\n\n\n\n\n\n\n  \n\n\n\n\nIndian News Headlines Dataset\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nSyamantak Basu\n\n\n\n\n\n\n  \n\n\n\n\nPARI Dataset Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\ncitizen journalism\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nSyamantak Basu\n\n\n\n\n\n\nNo matching items"
  }
]